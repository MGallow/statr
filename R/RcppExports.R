# Generated by using Rcpp::compileAttributes() -> do not edit by hand
# Generator token: 10BE3573-1514-4C36-9D1C-5A225CD40393

#' @title K fold (c++)
#' @description creates vector of shuffled indices
#'
#' @param n number of eleemtns
#' @param K number of folds
#' @return returns vector
#' @examples
#' kfold(10, 3)
#'
kfold <- function(n, K) {
    .Call('statr_kfold', PACKAGE = 'statr', n, K)
}

#' @title CV Logisticc (c++)
#' @description Computes the coefficient estimates for logistic regression. ridge regularization and bridge regularization optional. This function is to be used with the "logisticc" function.
#'
#' @param X matrix
#' @param y matrix or vector of response values 0,1
#' @param lam vector of tuning parameters for ridge regularization term. Defaults to `lam = 0`
#' @param alpha vector of tuning parameters for bridge regularization term. Defaults to 'alpha = 1.5'
#' @param penalty choose from c('none', 'ridge', 'bridge'). Defaults to 'none'
#' @param intercept Defaults to TRUE
#' @param method optimization algorithm. Choose from 'IRLS' or 'MM'. Defaults to 'IRLS'
#' @param tol tolerance - used to determine algorithm convergence. Defaults to 1e-5
#' @param maxit maximum iterations. Defaults to 1e5
#' @param vec optional vector to specify which coefficients will be penalized
#' @param init optional initialization for MM algorithm
#' @param criteria specify the criteria for cross validation. Choose from c("mse", "logloss", "misclass"). Defauls to "logloss"
#' @param K specify number of folds in cross validation, if necessary
#'
#' @return returns best lambda, best alpha, and cross validation errors
#' @export
#' @examples
#' CV_logisticc(X, y, lam = seq(0.1, 2, 0.1), alpha = c(1.1, 1.9, 0.1), penalty = "bridge", method = "MM", vec = c(0,1,1,1))
#'
CV_logisticc <- function(X, y, lam = 0L, alpha = 0L, penalty = "none", intercept = TRUE, method = "IRLS", tol = 1e-5, maxit = 1e4, vec = 0L, init = 0L, criteria = "logloss", K = 5L) {
    .Call('statr_CV_logisticc', PACKAGE = 'statr', X, y, lam, alpha, penalty, intercept, method, tol, maxit, vec, init, criteria, K)
}

#' @title CV Linearc (c++)
#' @description Computes the coefficient estimates for linear regression. ridge regularization and bridge regularization optional. This function is to be used with the "linearc" function
#'
#' @param X matrix
#' @param y matrix or vector of response values 0,1
#' @param lam vector of tuning parameters for ridge regularization term. Defaults to `lam = 0`
#' @param alpha vector of tuning parameters for bridge regularization term. Defaults to 'alpha = 1.5'
#' @param penalty choose from c('none', 'ridge', 'bridge'). Defaults to 'none'
#' @param intercept Defaults to TRUE
#' @param method optimization algorithm. Choose from 'IRLS' or 'MM'. Defaults to 'IRLS'
#' @param tol tolerance - used to determine algorithm convergence. Defaults to 1e-5
#' @param maxit maximum iterations. Defaults to 1e5
#' @param vec optional vector to specify which coefficients will be penalized
#' @param init optional initialization for MM algorithm
#' @param K specify number of folds in cross validation, if necessary
#' @return returns best lambda, best alpha, cv.errors
#' @export
#' @examples
#' CV_linearc(X, y, lam = seq(0.1, 2, 0.1), alpha = seq(1.1, 1.9, 0.1), penalty = "bridge", vec = c(0,1,1,1))
#'
CV_linearc <- function(X, y, lam = 0L, alpha = 0L, penalty = "none", weights = 0L, intercept = TRUE, kernel = FALSE, method = "SVD", tol = 1e-5, maxit = 1e4, vec = 0L, init = 0L, K = 5L) {
    .Call('statr_CV_linearc', PACKAGE = 'statr', X, y, lam, alpha, penalty, weights, intercept, kernel, method, tol, maxit, vec, init, K)
}

#' @title Logitc (c++)
#' @description Computes the logit for u
#' @param u some number
#' @return returns the logit of u
#' @examples
#' logit(X*beta)
#'
logitc <- function(u) {
    .Call('statr_logitc', PACKAGE = 'statr', u)
}

#' @title Gradient of Logistic Regression (IRLS) (c++)
#' @description Computes the gradient of logistic regression (optional ridge regularization term). We use this to determine if the KKT conditions are satisfied. This function is to be used with the 'IRLSc' function.
#'
#' @param betas estimates (includes intercept)
#' @param X matrix
#' @param y response vector of 0,1
#' @param lam tuning parameter for ridge regularization term
#' @param vec vector to specify which coefficients will be penalized
#' @return returns the gradient
#' @examples
#' gradient_IRLS_logistic(betas, X, y, lam = 0.1, vec = c(0,1,1,1))
#'
gradient_IRLS_logisticc <- function(betas, X, y, lam = 0, vec = 0L) {
    .Call('statr_gradient_IRLS_logisticc', PACKAGE = 'statr', betas, X, y, lam, vec)
}

#' @title Iterative Re-Weighted Least Squares (c++)
#' @description Computes the logistic regression coefficient estimates using the iterative re-weighted least squares (IRLS) algorithm. This function is to be used with the 'logisticc' function.
#'
#' @param betas beta estimates (includes intercept)
#' @param X matrix
#' @param y matrix or vector of response 0,1
#' @param lam tuning parameter for regularization term
#' @param penalty choose from c('none', 'ridge'). Defaults to 'none'
#' @param intercept Defaults to TRUE
#' @param tol tolerance - used to determine algorithm convergence
#' @param maxit maximum iterations
#' @param vec optional vector to specify which coefficients will be penalized
#' @return returns beta estimates (includes intercept), total iterations, and gradients.
#' @examples
#' IRLSc(X, y, lam = 0.1, penalty = "ridge", vec = c(0,1,1,1))
#'
IRLSc <- function(X, y, lam = 0, penalty = "none", intercept = TRUE, tol = 1e-5, maxit = 1e5, vec = 0L, init = 0L) {
    .Call('statr_IRLSc', PACKAGE = 'statr', X, y, lam, penalty, intercept, tol, maxit, vec, init)
}

#' @title Linearc (c++)
#' @description Computes the linear regression coefficient estimates (ridge and bridge penalization and weights, optional)
#' @param X matrix
#' @param y matrix
#' @param lam optional tuning parameter for ridge regularization term. Defaults to 'lam = 0'
#' @param alpha optional tuning parameter for bridge regularization term. Defaults to "alpha = 1.5"
#' @param penalty choose from c("none", "ridge", "bridge"). Defaults to "none"
#' @param weights optional vector of weights for weighted least squares
#' @param intercept add column of ones if not already present. Defaults to TRUE
#' @param kernel use linear kernel to compute ridge regression coefficeients. Defaults to TRUE when p >> n (for "SVD")
#' @param method optimization algorithm. Choose from "SVD" or "MM". Defaults to "SVD"
#' @param tol tolerance - used to determine algorithm convergence for "MM". Defaults to 10^-5
#' @param maxit maximum iterations for "MM". Defaults to 10^5
#' @param vec optional vector to specify which coefficients will be penalized
#' @param init optional initialization for MM algorithm
#' @return returns the coefficient estimates
#' @export
#' @examples
#' Weighted ridge regression
#' library(dplyr)
#' X = dplyr::select(iris, -c(Species, Sepal.Length))
#' y = dplyr::select(iris, Sepal.Length)
#' linearc(X, y, lam = 0.1, penalty = "ridge", weights = rep(1:150), vec = c(0,1,1,1))
#'
#' Kernelized ridge regression
#' linearc(X, y, lam = 0.1, penalty = "ridge", kernel = T, vec = c(0,1,1,1))
#'
linearc <- function(X, y, lam = 0, alpha = 1.5, penalty = "none", weights = 0L, intercept = TRUE, kernel = FALSE, method = "SVD", tol = 1e-5, maxit = 1e5, vec = 0L, init = 0L) {
    .Call('statr_linearc', PACKAGE = 'statr', X, y, lam, alpha, penalty, weights, intercept, kernel, method, tol, maxit, vec, init)
}

#' @title Logistic Regression (c++)
#' @description Computes the coefficient estimates for logistic regression. ridge regularization and bridge regularization optional.
#'
#' @param X matrix
#' @param y matrix or vector of response values 0,1
#' @param lam optional tuning parameter for ridge regularization term. Defaults to `lam = 0`
#' @param alpha optional tuning parameter for bridge regularization term. Defaults to 'alpha = 1.5'
#' @param penalty choose from c('none', 'ridge', 'bridge'). Defaults to 'none'
#' @param intercept Defaults to TRUE
#' @param method optimization algorithm. Choose from 'IRLS' or 'MM'. Defaults to 'IRLS'
#' @param tol tolerance - used to determine algorithm convergence. Defaults to 1e-5
#' @param maxit maximum iterations. Defaults to 1e5
#' @param vec optional vector to specify which coefficients will be penalized
#' @param init optional initialization for MM algorithm
#' @return returns beta estimates (includes intercept), total iterations, and gradients.
#' @export
#' @examples
#' Logistic Regression
#' library(dplyr)
#' X = as.matrix(dplyr::select(iris, -Species))
#' y = as.matrix(dplyr::select(iris, Species))
#' y = ifelse(y == 'setosa', 1, 0)
#' logisticc(X, y, vec = c(0,1,1,1))
#'
#' ridge Logistic Regression with IRLS
#' logisticc(X, y, lam = 0.1, penalty = 'ridge', vec = c(0,1,1,1))
#'
#' ridge Logistic Regression with MM
#' logisticc(X, y, lam = 0.1, penalty = 'ridge', method = 'MM', vec = c(0,1,1,1))
#'
#' bridge Logistic Regression
#' logisticc(X, y, lam = 0.1, alpha = 1.5, penalty = 'bridge', method = "MM", vec = c(0,1,1,1))
#'
logisticc <- function(X, y, lam = 0, alpha = 1.5, penalty = "none", intercept = TRUE, method = "IRLS", tol = 1e-5, maxit = 1e5, vec = 0L, init = 0L) {
    .Call('statr_logisticc', PACKAGE = 'statr', X, y, lam, alpha, penalty, intercept, method, tol, maxit, vec, init)
}

#' @title Gradient of Logistic Regression (MM) (c++)
#' @description Computes the gradient of logistic regression (optional ridge and bridge regularization terms). We use this to determine if the KKT conditions are satisfied. This function is to be used with the 'MMc' function.
#'
#' @param betas beta estimates (includes intercept)
#' @param X matrix
#' @param y response vector of 0,1
#' @param lam tuning parameter for ridge regularization term. Defaults to 'lam = 0'
#' @param alpha optional tuning parameter for bridge regularization term. Defaults to 'alpha = 1.5'
#' @param gamma indicator function. 'gamma = 1' for ridge, 'gamma = 0' for bridge. Defaults to 'gamma = 1'
#' @param vec vector to specify which coefficients will be penalized
#' @return returns the gradient
#' @examples
#' gradient_MM_logistic(betas, X, y, lam = 0.1, alpha = 1.5, vec = c(0,1,1,1))
#'
gradient_MM_logisticc <- function(betas, X, y, lam = 0, alpha = 1.5, gamma = 1, vec = 0L) {
    .Call('statr_gradient_MM_logisticc', PACKAGE = 'statr', betas, X, y, lam, alpha, gamma, vec)
}

#' @title Gradient of Linear Regression (MM) (c++)
#' @description Computes the gradient of linear regression (optional ridge and bridge regularization terms). We use this to determine if the KKT conditions are satisfied. This function is to be used with the 'MM_linearc' function.
#'
#' @param betas beta estimates (includes intercept)
#' @param X matrix
#' @param y response vector of 0,1
#' @param lam tuning parameter for ridge regularization term
#' @param alpha optional tuning parameter for bridge regularization term. Defaults to 'alpha = 1.5'
#' @param gamma indicator function. 'gamma = 1' for ridge, 'gamma = 0' for bridge. Defaults to 'gamma = 1'
#' @param vec vector to specify which coefficients will be penalized
#' @return returns the gradient
#' @examples
#' gradient_MM_linearc(betas, X, y, lam = 0.1, alpha = 1.5, penalty = 'bridge')
#'
gradient_MM_linearc <- function(betas, X, y, lam = 0, alpha = 1.5, gamma = 1, weights = 0L, vec = 0L) {
    .Call('statr_gradient_MM_linearc', PACKAGE = 'statr', betas, X, y, lam, alpha, gamma, weights, vec)
}

#' @title Logistic Majorize-Minimization function (c++)
#' @description This function utilizes the MM algorithm. It will be used to compute the logistic regression coefficient estimates. This function is to be used with the 'logisticc' function.
#'
#' @param X matrix
#' @param y matrix or vector of response 0,1
#' @param lam optional tuning parameter for ridge regularization term. Defaults to 'lam = 0'
#' @param alpha optional tuning parameter for bridge regularization term. Defaults to 'alpha = 1.5'
#' @param gamma gamma indicator function. 'gamma = 1' for ridge, 'gamma = 0' for bridge. Defaults to 'gamma = 1'
#' @param intercept defaults to TRUE
#' @param tol tolerance - used to determine algorithm convergence
#' @param maxit maximum iterations
#' @param vec optional vector to specify which coefficients will be penalized
#' @param init optional initialization for MM algorithm
#' @return returns beta estimates (includes intercept), total iterations, and gradients.
#' @examples
#' MMc(X, y)
#'
MMc <- function(X, y, lam = 0, alpha = 1.5, gamma = 1, intercept = TRUE, tol = 1e-5, maxit = 1e5, vec = 0L, init = 0L) {
    .Call('statr_MMc', PACKAGE = 'statr', X, y, lam, alpha, gamma, intercept, tol, maxit, vec, init)
}

#' @title Linear Majorize-Minimization function (c++)
#' @description This function utilizes the MM algorithm. It will be used to compute the linear regression coefficient estimates with optional regularization penalties. This function is to be used with the 'linearc' function.
#'
#' @param X matrix
#' @param y matrix
#' @param lam optional tuning parameter for ridge regularization term. Defaults to 'lam = 0'
#' @param alpha optional tuning parameter for bridge regularization term. Defaults to 'alpha = 1.5'
#' @param gamma gamma indicator function. 'gamma = 1' for ridge, 'gamma = 0' for bridge. Defaults to 'gamma = 1'
#' @param intercept defaults to TRUE
#' @param tol tolerance - used to determine algorithm convergence
#' @param maxit maximum iterations
#' @param vec optional vector to specify which coefficients will be penalized
#' @param init optional initialization for MM algorithm
#' @return returns beta estimates (includes intercept), total iterations, and gradients.
#' @examples
#' MM_linearc(X, y)
#'
MM_linearc <- function(X, y, lam = 0, alpha = 1.5, gamma = 1, weights = 0L, intercept = TRUE, tol = 1e-5, maxit = 1e5, vec = 0L, init = 0L) {
    .Call('statr_MM_linearc', PACKAGE = 'statr', X, y, lam, alpha, gamma, weights, intercept, tol, maxit, vec, init)
}

#' @title Predict Logistic Regression (c++)
#' @description Generates prediction for logistic regression
#'
#' @param betas matrix of coefficientts
#' @param X matrix of (new) observations
#' @param y matrix of response values 0,1
#' @return predictions and loss metrics
#' @export
#' @examples
#'
#' fitted = logisticr(X, y, lam = 0.1, penalty = 'ridge', method = 'MM')
#' predict_logisticr(fitted$coefficients, X)
#'
predict_logisticc <- function(betas, X, y = 0L) {
    .Call('statr_predict_logisticc', PACKAGE = 'statr', betas, X, y)
}

#' @title Predict Linear Regression
#' @description Generates prediction for linear regression
#'
#' @param betas 'linearr' object or matrix of betas
#' @param X matrix of (new) observations
#' @param y matrix of response values
#' @return predictions and loss metrics
#' @export
#' @examples
#'
#' fitted = linearr(X, y, penalty = "ridge")
#' predict_linearr(fitted$coefficients, X)
#'
predict_linearc <- function(betas, X, y = 0L) {
    .Call('statr_predict_linearc', PACKAGE = 'statr', betas, X, y)
}

#' @title Gradient of Linear Regression (c++)
#' @description Computes the gradient of linear regression (optional ridge regularization term). This function is to be used with the 'SVDc' function.
#'
#' @param beta estimates (includes intercept)
#' @param X matrix
#' @param y response vector of 0,1
#' @param lam tuning parameter for ridge regularization term
#' @param weights option vector of weights for weighted least squares
#' @param intercept add column of ones if not already present. Defaults to TRUE
#' @return returns the gradient
#' @examples
#' gradient_linearc(betas, X, y, lam = 0.1, weights = rep(1,150), intercept = TRUE)
#'
gradient_linearc <- function(betas, X, y, lam = 0, weights = 0L, intercept = TRUE) {
    .Call('statr_gradient_linearc', PACKAGE = 'statr', betas, X, y, lam, weights, intercept)
}

#' @title Linear Singular Value Decomposition (c++)
#' @description Computes the logistic regression coefficient estimates using SVD. This function is to be used with the 'linearc' function.
#'
#' @param X matrix
#' @param y matrix
#' @param lam optional tuning parameter for ridge regularization term. Defaults to 'lam = 0'
#' @param weights optional vector of weights for weighted least squares
#' @param intercept add column of ones if not already present. Defaults to TRUE
#' @param kernel use linear kernel to compute ridge regression coefficeients. Defaults to TRUE when p >> n (for "SVD")
#' @return returns beta estimates (includes intercept) and gradients.
#' @examples
#' SVDc(X, y, lam = 0.1 weights = rep(1, 150))
#'
SVDc <- function(X, y, lam = 0, weights = 0L, intercept = TRUE, kernel = FALSE) {
    .Call('statr_SVDc', PACKAGE = 'statr', X, y, lam, weights, intercept, kernel)
}

